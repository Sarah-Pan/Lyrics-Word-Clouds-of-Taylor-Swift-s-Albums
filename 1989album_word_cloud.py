# -*- coding: utf-8 -*-
"""1989Album_Word_Cloud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iegedP1F3Riu_5nfbvHZyYL29G8DdL0n
"""

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import nltk
from wordcloud import WordCloud

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}

r = 1
lyrics = []
for thing in range(16):
  url = 'https://mojim.com/twy105095x16xr.htm'
  url = url.replace('r',str(r))
  res = requests.get(url, headers=headers)
  soup = BeautifulSoup(res.text, 'html.parser')
  lyric = soup.select('.fsZx1')
  t = []
  for i in lyric:
    clean = i.text
    t.append(clean)
  lyrics.append(t)
  if thing == 15:
    break
  r += 1

lyric

lyrics

drop_chinese = []
for texts in lyrics:
  for every in texts:
    dc = re.sub('[\u4e00-\u9fff]','',every)
    drop_chinese.append(dc)

drop_chinese

clean_needed = []
for song in drop_chinese:
  #for text in song: 
    new = song.replace('\n','').replace('Taylor Swift','').replace('※ Mojim.com\u3000','')
    clean_needed.append(new)
    #clean_needed = "".join(clean_needed)
clean_needed

length = len(clean_needed)

middle_index = length // 2

first_half = clean_needed[:middle_index]
second_half = clean_needed[middle_index:]

clean_first_8 = []
for j in first_half:
  songs = j.split('[',1)
  song = songs.pop(-1)
  clean_first_8.append(songs)

clean_first_8

second_half[1]

df_1 = pd.DataFrame(clean_first_8)
df_1

df_2 = pd.DataFrame(second_half)
df_2

df = pd.concat([df_1,df_2],axis = 0)
df

lowercase = []
for line in df[0]:
    lowercase.append(line.lower())

lowercase

#stopwords setting
nltk.download('stopwords')
nltk_stopwords= nltk.corpus.stopwords.words('english')

#punctuation remove setting
import string
string.punctuation

from nltk import wordpunct_tokenize

final = []
for article in lowercase:
    punct_token = wordpunct_tokenize(article)
    #remove stopwords
    punct_token = [word for word in punct_token if word not in nltk_stopwords]
    #remove string.punctuation
    punct_token = [word for word in punct_token if word not in string.punctuation]
    #remove word that is not alphabat or number
    punct_token = [word for word in punct_token if word.isalnum()==True] #判定字詞是否為字母或數字
    final.append(punct_token)

final

word_count = {}
for sentence in final:
  for word in sentence:
    if word not in word_count:
      word_count[word] = 1
    else:
      word_count[word] += 1

#sort
pd.DataFrame([word_count]).T.sort_values(by=0,ascending=False).head(10)

import matplotlib.pyplot as plt
from wordcloud import WordCloud
wc = WordCloud(background_color="black", 
               max_words = 2000)        
with open("1989.txt", "w") as f1:
    for line in final:
        test=','.join(line)
        f1.write(test)
text = open("1989.txt").read()

wc.generate(text)
plt.imshow(wc)
plt.axis("off")
plt.figure(figsize=(10,6), dpi = 100)
plt.show()